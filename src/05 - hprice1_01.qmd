---
title: "Determinantes del precio de venta de las casas (I)"
---

## Objetivo

Estimar los parámetros de una **regresión hedónica** donde se explican los precios de venta de una muestra de casas en función de sus características (tamaño, número de habitaciones, etc.).

## Datos

Utilizaremos los datos de `hprice`, base de datos que está contenida en el paquete `wooldridge` y que proporciona información sobre las características y precios de venta de una muestra de casas situadas en el área urbana de Boston en 1990.

```{r}
library(wooldridge)

data(hprice1)

str(hprice1)
```

Trabajaremos con las variables:

- `price`: precio de venta de las casas, en miles de dólares.
- `sqrft`: tamaño de la casa en pies cuadrados.
- `lotsize`: tamaño de la parcela en pies cuadrados.
- `bdrms`: número de dormitorios.


Para facilitar la interpretación de los resultados, transformaremos las variables `sqrft` y `lotsize` cuyas unidades de medida son pies cuadrados. Dividiendo por $10{,}76$ obtenemos las áreas de las casas y las parcelas en metros cuadrados.
```{r}
hprice1$house_m2 <- hprice1$sqrft / 10.76
hprice1$lot_m2 <- hprice1$lotsize / 10.76
```


## Paquetes 

En esta sesión utilizaremos también otros paquetes además de `wooldridge`:
```{r}
#| message: false

library(ggformula)
library(sandwich)
library(lmtest)
library(car)
```

- `ggformula`: la función `gf_point` permite realizar gráficos para explorar la existencia de patrones heteroscedásticos.

- `sandwich`: proporciona la función `vcovHC` para obtener estimaciones de la matriz de covarianzas robustas a heteroscedasticidad.

- `lmtest`: la función `coeftest` se utiliza para obtener tablas de regresión con errores típicos robustos a heteroscedasticidad.

- `car`: la función `lht` sirve para contrastar hipótesis lineales sobre los parámetros y permite utilizar matrices de covarianzas robustas. 


## Estimación por MCO

Explicamos los precios de las casas utilizando las variables descritas anteriormente:
$$
 \mathit{price} = \beta_0 + \beta_1 \mathit{house\_m2} + \beta_2 \mathit{lot\_m2} + \beta_3 \mathit{bdrms} + u 
$$
Esperamos que los parámetros $\beta_1$ y $\beta_2$ sean positivos y que se vendan más caras las casas más grandes o con parcelas mayores, manteniendo constante el resto de parámetros. El signo esperado del parámetro $\beta_3$ es menos claro. Al menos para valores moderados de $\mathit{bdrms}$ también tendría signo positivo. Pero esta variable podría tener un efecto negativo sobre el precio de venta si hay un número excesivo de habitaciones.

Estimación por MCO:
```{r ols}
mod1 <- lm(price ~ house_m2 + lot_m2 + bdrms, data = hprice1)
summary(mod1)
```

De acuerdo con estas estimaciones las tres pendientes tienen los signos esperados pero solo las variables que recogen el tamaño de la casa y la parcela tendrían efectos significativos al $5\%$. 


Sin embargo, un gráfico de los residuos y las predicciones parece indicar que no se cumple el supuesto de homoscedasticidad. Si este fuera el caso, los errores típicos y los contrastes usuales dejarían de ser válidos.
```{r}
#| fig-align: center

uhat1 <- resid(mod1)
yhat1 <- fitted(mod1)
gf_point(uhat1 ~ yhat1)
```


## Matriz de covarianzas robusta

La función `coeftest` permite presentar los resultados de la estimación de MCO usando una matriz de covarianzas robusta a heteroscedasticidad mediante el argumento `vcov.`.  Para estimar esta matriz utilizamos la función `vcovHC`:
```{r}
coeftest(mod1, vcov. = vcovHC) 
```

La tabla generada con `coeftest(mod1, vcovHC)` incluye contrastes $t$ robustos con los que podemos determinar la significación individual de los parámetros del modelo de regresión y son válidos haya o no haya problemas de heteroscedasticidad. 

Para facilitar la comparación, en la siguiente tabla se reproducen los resultados de MCO con los errores típicos usuales, que solo son válidos con homoscedasticidad, y con errores típicos robustos a heteroscedasticidad:
```{r}
#| echo: false 
#| message: false

library(modelsummary)
modelsummary(list("MCO" = mod1, "MCO + robust S.E." = mod1), 
             vcov = c("constant", "robust"),
             stars = c("*" = 0.1, "**" = 0.05, "***" = 0.01), 
             gof_omit = ".*")
```
Las estimaciones de los parámetros son las mismas en los dos casos pero los errores típicos robustos son, en este caso,  mucho mayores que los obtenidos con las fórmulas habituales de MCO. Cuando se usa un contraste robusto a heteroscedasticidad, el efecto de la variable $\mathit{lot\_m2}$ no es significativamente distinto de $0$. 


## Contrastes de hipótesis robustos

Para contrastar hipótesis lineales generales sobre los parámetros del modelo utilizaremos la función `lht`. El primer argumento es el modelo para el que construimos los contrastes. El segundo argumento especifica la hipótesis que queremos contrastar. En nuestro caso contrastaremos la hipótesis de que $\mathit{lot\_m2}$ y $\mathit{bdrms}$ no son conjuntamente significativas:
$$
H_0\!: \beta_2 = \beta_3 = 0
$$
En casos como este, donde queremos contrastar la significación de un conjunto de regresores, simplemente podemos pasar a la función `lht` un vector con los nombres de las variables cuyas pendientes son 0 bajo la hipótesis nula.
```{r names-h0} 
names_h0 <- c("lot_m2", "bdrms")
```

Si no indicamos nada más, la función `lht` calcula un contraste de Wald usando la matriz de covarianzas de MCO:
```{r wald-test}
lht(mod1, names_h0)
```
Los resultados obtenidos permitirían rechazar la hipótesis nula a un nivel de significación del $5\%$. Sin embargo, el contraste realizado solo es valido si se cumple el supuesto de homoscedasticidad. Para obtener un contraste robusto a heteroscedasticidad, añadimos el argumento `vcov. = vcovHC`:
```{r wald-test-robust}
lht(mod1, names_h0, vcov. = vcovHC)
```
El contraste robusto a heteroscedasticidad toma un valor muy bajo, por lo que no se podría rechazar la hipótesis de que los parámetros de las variables $\mathit{lot\_m2}$ y $\mathit{bdrms}$ son iguales a $0$ para un nivel de significación del $5\%$.


## Contrastes de hetroscedasticidad

Los contrastes de heteroscedasticidad se calculan a partir de una regresión auxiliar cuya variable dependiente son los residuos al cuadrado:
```{r}
uhat1_sq <- uhat1 ^ 2
```

Podemos examinar un gráfico de los residuos al cuadrado con respecto de las predicciones como una prueba informal de heteroscedasticidad:
```{r}
#| fig-align: "center"

gf_point(uhat1_sq ~ yhat1)
```

El gráfico parece indicar que la varianza de los residuos $\hat{u}$ es mayor cuanto mayor es el valor de las predicciones, $\hat{y}$.


### Contraste de Breusch-Pagan

Estimamos una regresión auxiliar donde la variable dependiente son los residuos al cuadrado y se incluyen los mismos regresores que en el modelo de regresión:
```{r aux-bp}
aux_bp <- lm(uhat1_sq ~ house_m2 + lot_m2 + bdrms, data = hprice1)
summary(aux_bp)
```

El contraste $F$ de la regresión es significativo al $5\%$ por lo que rechazaríamos la hipótesis nula de homoscedasticidad.

Con un poco más de trabajo podemos calcular el contraste $\mathrm{LM}$ multiplicando el número de observaciones por el $R^2$ de la regresión auxiliar: $\mathrm{LM} = 88 \cdot 0.1601 = `r round(88 * 0.1601, 1)`$. Bajo la hipótesis nula el contraste $\mathrm{LM}$ se distribuye como una $\chi^2$ con $3$ grados de libertad. Podemos obtener el valor-$p$ de este contraste con la instrucción:
```{r lm-pvalue}
pchisq(14.1, 3, lower.tail = FALSE)
```
Tanto la forma $F$ como la forma $\chi^2$ del contraste de Breusch-Pagan muestran fuerte evidencia en contra de la hipótesis de homoscedasticidad.

### Contraste de White

El contraste de White incluye más términos en la regresión auxiliar que el contraste de Breusch y Pagan. A las variables explicativas se añaden sus cuadrados y productos cruzados. En primer lugar, construimos esos regresores adicionales:
```{r white-regr}
hprice1$lot_m2_sq <- hprice1$lot_m2 ^ 2
hprice1$house_m2_sq <- hprice1$house_m2 ^ 2
hprice1$bdrms_sq <- hprice1$bdrms ^ 2
hprice1$lot_m2_house_m2 <- hprice1$lot_m2 * hprice1$house_m2
hprice1$lot_m2_bdrms <- hprice1$lot_m2 * hprice1$bdrms
hprice1$house_m2_bdrms <- hprice1$house_m2 * hprice1$bdrms
```
En segundo lugar, estimamos la regresión auxiliar:
```{r white-aux}
aux_white <- lm(uhat1_sq ~ house_m2 + lot_m2 + bdrms + 
                  lot_m2_sq +  house_m2_sq + bdrms_sq +
                  lot_m2_house_m2 + lot_m2_bdrms + house_m2_bdrms,
                data = hprice1)
summary(aux_white)
```
El estadístico $F$ de la regresión toma el valor $5.4$ que es muy elevado para provenir de una $F$ con $9$ y $78$ grados de libertad. Por tanto, se rechazaría la hipótesis nula de homoscedasticidad a un nivel de significación del $5\%$. 

### Alternativas al contraste de White

El contraste de White utiliza muchos términos en la regresión auxiliar lo que le puede restar potencia, sobre todo cuando el modelo de regresión incluye muchas variables explicativas. Para reducir el número de parámetros a estimar, a veces no se incluyen los productos cruzados en la regresión auxiliar.

La alternativa propuesta por Wooldridge reduce a $2$ el número de pendientes en la regresión auxiliar, independientemente de cuantas explicativas aparezcan en el modelo de regresión. La regresión auxiliar tendría como únicos regresores las predicciones y las predicciones al cuadrado:
```{r white-alt}
yhat1_sq <- yhat1 ^ 2
aux_white2 <- lm(uhat1_sq ~  yhat1 + yhat1_sq, data = hprice1)
summary(aux_white2)
```

Al igual que con los otros contrastes, se encuentra fuerte evidencia en contra de la hipótesis nula de homoscedasticidad.

